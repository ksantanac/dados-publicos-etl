# ğŸ“Š Pipeline ETL - Receita Federal

Pipeline orquestrado pelo **Airflow (GCP Composer)** que processa dados da Receita Federal seguindo o padrÃ£o da arquitetura **MedalhÃ£o**.

---

## ğŸ“Œ Fluxo da DAG

1. **ExtraÃ§Ã£o (Raw)**
   - Scraping do site da Receita Federal com `BeautifulSoup`
   - Armazenamento no GCS:  
     `gs://{bucket}/raw/`

2. **TransformaÃ§Ã£o (Trusted)**
   - Job PySpark no Dataproc:
     - Limpeza de dados  
     - PadronizaÃ§Ã£o de campos
   - SaÃ­da em GCS:  
     `gs://{bucket}/trusted/`

3. **FormataÃ§Ã£o para Excel**
   - ConversÃ£o para CSV com:
     - Encoding `UTF-8 BOM`
     - Delimitadores compatÃ­veis
   - SaÃ­da em:  
     `gs://{bucket}/trusted/`

4. **OtimizaÃ§Ã£o (Refined)**
   - ConversÃ£o para `Parquet` com:
     - Particionamento  
     - CompressÃ£o `Snappy`
   - SaÃ­da em:  
     `gs://{bucket}/refined/`

---

## âš™ï¸ Estrutura do Projeto

```bash
.
â”œâ”€â”€ dag.py                      # DAG principal
â””â”€â”€ src/
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ gcp_conn.py         # ConfiguraÃ§Ã£o de conexÃµes GCP
    â”‚   â””â”€â”€ dag-config.toml     # ConfiguraÃ§Ãµes da DAG e variaveis
    â”œâ”€â”€ scripts/
    â”‚   â””â”€â”€ script.py           # Job PySpark
    â””â”€â”€ utils/
        â”œâ”€â”€ scraping.py         # FunÃ§Ã£o de scraping
        â””â”€â”€ add_bom.py          # FunÃ§Ã£o para adicionar BOM no CSV
```

---

## ğŸ”§ PrÃ©-requisitos

### â˜ï¸ GCP:
- Composer 2.x
- Cluster Dataproc configurado
- Bucket no GCS com as seguintes pastas:
  - `raw/`
  - `trusted/`
  - `refined/`

### ğŸ Python:
`requirements.txt`:

```txt
beautifulsoup4==4.12.0
pyspark==3.5.0
google-cloud-storage==2.10.0
```

---

## ğŸš€ ImplantaÃ§Ã£o

### Configurar variÃ¡veis no Airflow:

```bash
airflow variables set GCP_PROJECT_ID seu-projeto
airflow variables set GCS_BUCKET seu-bucket
```

### Carregar a DAG:

```bash
gcloud composer environments storage dags import \
    --environment=composer-env \
    --location=us-central1 \
    --source=dags/receita_federal_etl.py
```

---

## ğŸ“ Outputs Esperados

| Camada             | Formato   | LocalizaÃ§Ã£o                                |
|--------------------|-----------|---------------------------------------------|
| Raw                | Original  | `gs://{bucket}/raw/`                        |
| Trusted            | CSV       | `gs://{bucket}/trusted/`                   |
| Trusted (Excel)    | CSV       | `gs://{bucket}/trusted/receita_federal_excel/` |
| Refined            | Parquet   | `gs://{bucket}/refined/`                   |

---

## ğŸ§¾ VersÃ£o Minimalista

```markdown
# ETL Receita Federal

**Fluxo**: Scraping â†’ Raw â†’ PySpark â†’ Trusted â†’ Excel/Parquet â†’ Refined

**Infra**:
- Airflow (Composer)
- Dataproc (PySpark)
- GCS (Raw/Trusted/Refined)

**ImplantaÃ§Ã£o**:
1. Setar variÃ¡veis no Airflow
2. Upload da DAG via `gcloud composer`

**Contatos**: [kauesantana_13@hotmail.com]
```

---

ğŸ“« **Contato**: [kauesantana_13@hotmail.com]  
ğŸ”— **GCP Composer**, **Dataproc**, **GCS**, **PySpark**, **Airflow**, **BeautifulSoup**
